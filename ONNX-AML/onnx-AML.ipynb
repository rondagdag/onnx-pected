{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a deep learning model\n",
    "\n",
    "adopted from: [Azure Machine Learning QuickStarts from SollianceNet](https://github.com/solliancenet/azure-machine-learning-quickstarts/blob/master/aml-python-sdk/starter-artifacts/python-notebooks/05-aml-onnx/solution-onnx-AML.ipynb)\n",
    "\n",
    "In this notebook you will train a deep learning model to classify the descriptions of car components as compliant or non-compliant. \n",
    "\n",
    "Each document in the supplied training data set is a short text description of the component as documented by an authorized technician. \n",
    "The contents include:\n",
    "- Manufacture year of the component (e.g. 1985, 2010)\n",
    "- Condition of the component (poor, fair, good, new)\n",
    "- Materials used in the component (plastic, carbon fiber, steel, iron)\n",
    "\n",
    "The compliance regulations dictate:\n",
    "*Any component manufactured before 1995 or in fair or poor condition or made with plastic or iron is out of compliance.*\n",
    "\n",
    "For example:\n",
    "* Manufactured in 1985 made of steel in fair condition -> **Non-compliant**\n",
    "* Good condition carbon fiber component manufactured in 2010 -> **Compliant**\n",
    "* Steel component manufactured in 1995 in fair condition -> **Non-Compliant**\n",
    "\n",
    "The labels present in this data are 0 for compliant, 1 for non-compliant.\n",
    "\n",
    "The challenge with classifying text data is that deep learning models only undertand vectors (e.g., arrays of numbers) and not text. To encode the car component descriptions as vectors, we use an algorithm from Stanford called [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/). GloVe provides us pre-trained vectors that we can use to convert a string of text into a vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Azure Machine Learning resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Azure Machine Learning SDK provides a comprehensive set of a capabilities that you can use directly within a notebook including:\n",
    "- Creating a **Workspace** that acts as the root object to organize all artifacts and resources used by Azure Machine Learning.\n",
    "- Creating **Experiments** in your Workspace that capture versions of the trained model along with any desired model performance telemetry. Each time you train a model and evaluate its results, you can capture that run (model and telemetry) within an Experiment.\n",
    "- Creating **Compute** resources that can be used to scale out model training, so that while your notebook may be running in a lightweight container in Azure Notebooks, your model training can actually occur on a powerful cluster that can provide large amounts of memory, CPU or GPU. \n",
    "- Using **Automated Machine Learning (AutoML)** to automatically train multiple versions of a model using a mix of different ways to prepare the data and different algorithms and hyperparameters (algorithm settings) in search of the model that performs best according to a performance metric that you specify. \n",
    "- Packaging a Docker **Image** that contains everything your trained model needs for scoring (prediction) in order to run as a web service.\n",
    "- Deploying your Image to either Azure Kubernetes or Azure Container Instances, effectively hosting the **Web Service**.\n",
    "\n",
    "In Azure Notebooks, all of the libraries needed for Azure Machine Learning are pre-installed. To use them, you just need to import them. Run the following cell to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import json\n",
    "import azureml\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.models import model_from_json\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "To begin, you will need to provide the following information about your Azure Subscription.\n",
    "\n",
    "**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** Note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace).\n",
    "\n",
    "**If an environment is provided to you be sure to replace XXXXX in the values below with your unique identifier.**\n",
    "\n",
    "In the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n",
    "\n",
    "To get these values, do the following:\n",
    "1. Navigate to the Azure Portal and login with the credentials provided.\n",
    "2. From the left hand menu, under Favorites, select `Resource Groups`.\n",
    "3. In the list, select the resource group with the name similar to `XXXXX`.\n",
    "4. From the Overview tab, capture the desired values.\n",
    "\n",
    "Execute the following cell by selecting the `>|Run` button in the command bar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"\" # <- needs to be the subscription with the Quick-Starts resource group\n",
    "\n",
    "#Provide values for the existing Resource Group \n",
    "resource_group = \"Quick-Starts-XXXXX\" # <- replace XXXXX with your unique identifier\n",
    "\n",
    "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
    "workspace_name = \"quick-starts-ws-XXXXX\" # <- replace XXXXX with your unique identifier\n",
    "workspace_region = \"eastus\" # <- region of your Quick-Starts resource group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_folder = 'deploy'\n",
    "onnx_export_folder = 'onnx'\n",
    "\n",
    "# this is the URL to the CSV file containing the GloVe vectors\n",
    "#glove_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "#             'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "#             'quickstarts/connected-car-data/glove.6B.100d.txt')\n",
    "\n",
    "# this is the URL to the CSV file containing the care component descriptions\n",
    "data_url = ('https://onnxpected.blob.core.windows.net/'\n",
    "            'quickstarts/connected-car_components.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://onnxpected.blob.core.windows.net/quickstarts/connected-car_components.csv\n"
     ]
    }
   ],
   "source": [
    "# print(glove_url)\n",
    "print(data_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the car components labeled data\n",
    "car_components_df = pd.read_csv(data_url)\n",
    "components = car_components_df[\"text\"].tolist()\n",
    "labels = car_components_df[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65 unique tokens.\n",
      "Shape of data tensor: (100000, 100)\n",
      "Shape of label tensor: (100000,)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 100                                           \n",
    "training_samples = 90000                                 \n",
    "validation_samples = 5000    \n",
    "max_words = 10000      \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(components)\n",
    "sequences = tokenizer.texts_to_sequences(components)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])                     \n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "x_test = data[training_samples + validation_samples:]\n",
    "y_test = labels[training_samples + validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model from model.h5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras model is saved in model.h5 file. Load a previously trained Keras model from the local **model** directory and review the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0907 08:09:53.349914 33108 deprecation_wrapper.py:119] From C:\\Users\\ron\\Anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0907 08:09:56.290526 33108 deprecation_wrapper.py:119] From C:\\Users\\ron\\Anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0907 08:09:56.302559 33108 deprecation_wrapper.py:119] From C:\\Users\\ron\\Anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0907 08:10:11.817027 33108 deprecation_wrapper.py:119] From C:\\Users\\ron\\Anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0907 08:10:11.818032 33108 deprecation_wrapper.py:119] From C:\\Users\\ron\\Anaconda3\\envs\\RecSys\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disk.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "if cwd.endswith('/deploy'):\n",
    "    os.chdir('../')\n",
    "\n",
    "embedding_dim = 100\n",
    "maxlen = 100                                             \n",
    "max_words = 10000    \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# download the pretrained model from the deep learning quickstart\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "model_url = ('https://onnxpected.blob.core.windows.net/'\n",
    "            'quickstarts/model.h5')\n",
    "\n",
    "urllib.request.urlretrieve(model_url, os.path.join('./model', 'model.h5'))\n",
    "\n",
    "# load weights into new model\n",
    "model.load_weights(os.path.join('./model', 'model.h5'))\n",
    "print(\"Model loaded from disk.\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a Keras model to ONNX\n",
    "In the steps that follow, you will convert Keras model you just trained to the ONNX format. This will enable you to use this model for classification in a very broad range of environments, outside of Azure Databricks including:\n",
    "\n",
    "- Web services \n",
    "- iOS and Android mobile apps\n",
    "- Windows apps\n",
    "- IoT devices\n",
    "\n",
    "Convert the model to ONNX by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://onnxpected.blob.core.windows.net/quickstarts/model.h5\n"
     ]
    }
   ],
   "source": [
    "print(model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0907 08:10:12.233025 33108 deprecation_wrapper.py:119] From C:\\Users\\ron\\Anaconda3\\envs\\RecSys\\lib\\site-packages\\keras2onnx\\common\\utils.py:42: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W0907 08:10:12.233025 33108 deprecation_wrapper.py:119] From C:\\Users\\ron\\Anaconda3\\envs\\RecSys\\lib\\site-packages\\keras2onnx\\common\\utils.py:42: The name tf.logging.WARN is deprecated. Please use tf.compat.v1.logging.WARN instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnxmltools\n",
    "\n",
    "# Convert the Keras model to ONNX\n",
    "onnx_model_name = 'component_compliance.onnx'\n",
    "converted_model = onnxmltools.convert_keras(model, onnx_model_name, target_opset=7)\n",
    "\n",
    "# Save the model locally...\n",
    "onnx_model_path = os.path.join(deployment_folder, onnx_export_folder)\n",
    "os.makedirs(onnx_model_path, exist_ok=True)\n",
    "onnxmltools.utils.save_model(converted_model, os.path.join(onnx_model_path,onnx_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell created a new file called `component_compliance.onnx` that contains the ONNX version of the model. \n",
    "\n",
    "To be able to use an ONNX model for inferencing, your environment only needs to have the `onnxruntime` installed. Run the following cell to install the `onnxruntime`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in c:\\users\\ron\\anaconda3\\envs\\recsys\\lib\\site-packages (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "#%%sh\n",
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try using this ONNX model to classify a component description by running the following cell. Remeber the prediction will be a value close to 0 (non-compliant) or to 1 (compliant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare ONNX Inference Performace with Keras\n",
    "\n",
    "Create an onnxruntime InferenceSession and observe the expected input shape for inference. Classify a sample data from test set using both ONNX and Keras. Remeber the prediction will be a value close to 0 (non-compliant) or to 1 (compliant).\n",
    "\n",
    "Next, we will evaluate the performance of ONNX and Keras by running the same sample 10,000 times. You will observe that ONNX is approximately 10 times faster than Keras in making inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input shape:  [None, 100]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "# Load the ONNX model and observe the expected input shape\n",
    "onnx_session = onnxruntime.InferenceSession(\n",
    "    os.path.join(os.path.join(deployment_folder, onnx_export_folder), onnx_model_name))\n",
    "input_name = onnx_session.get_inputs()[0].name\n",
    "output_name = onnx_session.get_outputs()[0].name\n",
    "print('Expected input shape: ', onnx_session.get_inputs()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (1, 100)\n",
      "ONNX prediction:  [array([[0.]], dtype=float32)]\n",
      "Keras prediction:  [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Grab one sample from the test data set\n",
    "x_test_float = np.reshape(x_test[1502].astype(np.float32), (1,100))\n",
    "# Confirm that the input shape is same as expected input shape\n",
    "print('Input shape: ', x_test_float.shape)\n",
    "\n",
    "# Run an ONNX session to classify the sample.\n",
    "print('ONNX prediction: ', onnx_session.run([output_name], {input_name : x_test_float}))\n",
    "\n",
    "# Use Keras to make predictions on the same sample\n",
    "print('Keras prediction: ', model.predict(x_test_float))\n",
    "\n",
    "# Next we will compare the performance of ONNX vs Keras\n",
    "import timeit\n",
    "n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras performance:  4.159399100004521\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "for i in range(n):\n",
    "    model.predict(x_test_float)\n",
    "keras_elapsed = timeit.default_timer() - start_time\n",
    "print('Keras performance: ', keras_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX performance:  1.0471896000017296\n",
      "ONNX is about 4 times faster than Keras\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "for i in range(n):\n",
    "    onnx_session.run([output_name], {input_name : x_test_float})\n",
    "onnx_elapsed = timeit.default_timer() - start_time\n",
    "print('ONNX performance: ', onnx_elapsed)\n",
    "print('ONNX is about {} times faster than Keras'.format(round(keras_elapsed/onnx_elapsed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Deep Learning ONNX format model as a web service\n",
    "To demonstrate one example of using the ONNX format model in a new environment, you will deploy the ONNX model to a webservice. On the web server, the only component required by the model is the ONNX Runtime, which is used to load the model and use it for scoring. Neither Keras nor TensorFlow are required on the web server.\n",
    "\n",
    "In this case, you will use the Azure Machine Learning service SDK to programmatically create a Workspace, register your model, create a container image for the web service that uses it and deploy that image on to an Azure Container Instance.\n",
    "\n",
    "Run the following cells to create some helper functions that you will use for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "def getOrCreateWorkspace(subscription_id, resource_group, workspace_name, workspace_region):\n",
    "# if you want to create a new workspace, comment Workspace.from_config() and uncomment ws= Workspace.create\n",
    "    ws = Workspace.from_config()\n",
    "    \n",
    "# By using the exist_ok param, if the workspace already exists we get a reference to the existing workspace instead of an error\n",
    "\n",
    "#ws = Workspace.create(\n",
    "#    name = workspace_name,\n",
    "#    subscription_id = subscription_id,\n",
    "#    resource_group = resource_group, \n",
    "#    location = workspace_region,\n",
    "#    exist_ok = True)\n",
    "    \n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deployModelAsWebService(ws, model_folder_path=\"models\", model_name=\"component_compliance\", \n",
    "                scoring_script_filename=\"scoring_service.py\", \n",
    "                conda_packages=['numpy','pandas'],\n",
    "                pip_packages=['azureml-sdk','onnxruntime'],\n",
    "                conda_file=\"dependencies.yml\", runtime=\"python\",\n",
    "                cpu_cores=1, memory_gb=1, tags={'name':'scoring'},\n",
    "                description='Compliance classification web service.',\n",
    "                service_name = \"complianceservice\"\n",
    "               ):\n",
    "    # notice for the model_path, we supply the name of the outputs folder without a trailing slash\n",
    "    # this will ensure both the model and the customestimators get uploaded.\n",
    "    print(\"Registering and uploading model...\")\n",
    "    registered_model = Model.register(model_path=model_folder_path, \n",
    "                                      model_name=model_name, \n",
    "                                      workspace=ws)\n",
    "\n",
    "    # create a Conda dependencies environment file\n",
    "    print(\"Creating conda dependencies file locally...\")\n",
    "    from azureml.core.conda_dependencies import CondaDependencies \n",
    "    mycondaenv = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "    with open(conda_file,\"w\") as f:\n",
    "        f.write(mycondaenv.serialize_to_string())\n",
    "        \n",
    "    # create container image configuration\n",
    "    print(\"Creating container image configuration...\")\n",
    "    from azureml.core.image import ContainerImage\n",
    "    image_config = ContainerImage.image_configuration(execution_script = scoring_script_filename,\n",
    "                                                      runtime = runtime,\n",
    "                                                      conda_file = conda_file)\n",
    "    \n",
    "    # create ACI configuration\n",
    "    print(\"Creating ACI configuration...\")\n",
    "    from azureml.core.webservice import AciWebservice, Webservice\n",
    "    aci_config = AciWebservice.deploy_configuration(\n",
    "        cpu_cores = cpu_cores, \n",
    "        memory_gb = memory_gb, \n",
    "        tags = tags, \n",
    "        description = description)\n",
    "\n",
    "    # deploy the webservice to ACI\n",
    "    print(\"Deploying webservice to ACI...\")\n",
    "    webservice = Webservice.deploy_from_model(\n",
    "      workspace=ws, \n",
    "      name=service_name, \n",
    "      deployment_config=aci_config,\n",
    "      models = [registered_model], \n",
    "      image_config=image_config\n",
    "    )\n",
    "    webservice.wait_for_deployment(show_output=True)\n",
    "    \n",
    "    return webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your web service which knows how to load the model and use it for scoring needs saved out to a file for the Azure Machine Learning service SDK to deploy it. Run the following cell to create this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deploy/scoring_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $deployment_folder/scoring_service.py\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from azureml.core.model import Model\n",
    "import onnxruntime\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    \n",
    "    try:\n",
    "        model_path = Model.get_model_path('component_compliance')\n",
    "        model_file_path = os.path.join(model_path,'component_compliance.onnx')\n",
    "        print('Loading model from: ', model_file_path)\n",
    "        \n",
    "        # Load the ONNX model\n",
    "        model = onnxruntime.InferenceSession(model_file_path)\n",
    "        print('Model loaded...')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        print(\"Received input: \", raw_data)\n",
    "        \n",
    "        input_data = np.array(json.loads(raw_data)).astype(np.float32)\n",
    "        \n",
    "        # Run an ONNX session to classify the input.\n",
    "        result = model.run(None, {model.get_inputs()[0].name:input_data})[0]\n",
    "        result = result[0][0].item()\n",
    "        \n",
    "        # return just the classification index (0 or 1)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create your Workspace (or retrieve the existing one if it already exists) and deploy the model as a web service.\n",
    "\n",
    "Run the next two cells to perform the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0907 08:10:23.053494 33108 authentication.py:502] Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger(\"adal-python\").setLevel(logging.WARN)\n",
    "\n",
    "ws =  getOrCreateWorkspace(subscription_id, resource_group, \n",
    "                   workspace_name, workspace_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complianceservice-zst AciWebservice(workspace=Workspace.create(name='onnx-pected-ws', subscription_id='8a943a54-0eb6-4379-b766-3c5e997ddc14', resource_group='onnx-pected-rg'), name=complianceservice-zst, image_id=complianceservice-zst:2, compute_type=ACI, state=None, scoring_uri=http://5ce5573f-7e6d-456e-985a-0b772f87eddc.southcentralus.azurecontainer.io/score, tags={'name': 'scoring'}, properties={'azureml.git.repository_uri': 'https://github.com/rondagdag/onnx-pected.git', 'mlflow.source.git.repoURL': 'https://github.com/rondagdag/onnx-pected.git', 'azureml.git.branch': 'master', 'mlflow.source.git.branch': 'master', 'azureml.git.commit': '1a9b1e0be9eab7a021439166427a9c4ff68644d1', 'mlflow.source.git.commit': '1a9b1e0be9eab7a021439166427a9c4ff68644d1', 'azureml.git.dirty': 'True'})\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "webservice_name = \"complianceservice-zst\"\n",
    "\n",
    "# It is important to change the current working directory so that your generated scoring-service.py is at the root\n",
    "# This is required by the Azure Machine Learning SDK\n",
    "os.chdir(deployment_folder)\n",
    "\n",
    "webservice = None\n",
    "for service in Webservice.list(ws):\n",
    "    if (service.name == webservice_name):\n",
    "        webservice = service\n",
    "        print(webservice.name, webservice)\n",
    "\n",
    "if webservice == None:\n",
    "    print(os.getcwd())\n",
    "    webservice = deployModelAsWebService(ws, model_folder_path=onnx_export_folder, \n",
    "                                     model_name=\"component_compliance\", service_name = webservice_name)\n",
    "\n",
    "# Change the directory back...\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your deployed web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a sample from the test data set to send\n",
    "# I encourage you to change 1502 to 1300 or some data from test\n",
    "test_sample = np.reshape(x_test.astype(np.float32)[1502], (1,100))\n",
    "test_sample_json = json.dumps(test_sample.tolist())\n",
    "\n",
    "# invoke the web service\n",
    "result = webservice.run(input_data=test_sample_json)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume the Deployed Web Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring URI is: http://5ce5573f-7e6d-456e-985a-0b772f87eddc.southcentralus.azurecontainer.io/score\n",
      "Predictions\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = webservice.scoring_uri\n",
    "print('Scoring URI is: {}'.format(url))\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "response = requests.post(url, test_sample_json, headers=headers)\n",
    "print('Predictions')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a working web service deployed that uses the ONNX version of your Keras deep learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Deep Learning",
  "notebookId": 2340934485665719
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
